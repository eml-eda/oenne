{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "LI4-dGlWtHMM",
   "metadata": {
    "id": "LI4-dGlWtHMM"
   },
   "source": [
    "**TODO**: Remove below (drive-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0GNlwGsfPe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa0GNlwGsfPe",
    "outputId": "a80b7116-3886-498a-ea8b-13150affcfa0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#!ln -s /content/drive/MyDrive/Didattica/OENNE_notebooks/utils ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a35c97-8b34-4c38-a930-3d709420609f",
   "metadata": {},
   "source": [
    "**TODO**: Remove below (server-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06f6deb-a721-4836-8b00-a329f2ba9906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f978ca1",
   "metadata": {
    "id": "0f978ca1"
   },
   "source": [
    "## Hands-on #3: Quantization with PLiNIO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2bcfe",
   "metadata": {
    "id": "59c2bcfe"
   },
   "source": [
    "In this notebook, you will:\n",
    "1. Load the optimized and pruned DNN found at the end of Hands-on #2\n",
    "2. Apply Quantization-Aware Training (QAT) to it.\n",
    "3. Export the final model in an ONNX format compatible with the AI Compiler that you will use in Hands-on #4.\n",
    "\n",
    "Considering the flow seen in class, we are here:\n",
    "\n",
    "![qat.png]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3da17-365b-4f04-86ac-a6dd0b7aac90",
   "metadata": {
    "id": "fcf3da17-365b-4f04-86ac-a6dd0b7aac90"
   },
   "source": [
    "# Part 0: Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8e445-aa06-43d3-8ad9-059be3da58c3",
   "metadata": {
    "id": "58d8e445-aa06-43d3-8ad9-059be3da58c3"
   },
   "source": [
    "As usual, we start by importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4a45bae-11ce-4318-9386-f74cb68ca0f0",
   "metadata": {
    "id": "a4a45bae-11ce-4318-9386-f74cb68ca0f0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from plinio.cost import params_bit\n",
    "from plinio.methods import MPS\n",
    "from plinio.methods.mps import get_default_qinfo\n",
    "from plinio.methods.mps.quant.quantizers import PACTAct\n",
    "from plinio.methods.mps.quant.backends import Backend, integerize_arch\n",
    "from plinio.methods.mps.quant.backends.match import MATCHExporter\n",
    "\n",
    "import pytorch_benchmarks.image_classification as icl\n",
    "from pytorch_benchmarks.utils import CheckPoint, EarlyStopping\n",
    "\n",
    "from utils.train import set_seed, try_load_checkpoint\n",
    "from utils.plot import plot_learning_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9e0f1-3de5-4840-ab8a-d89176a4bd74",
   "metadata": {
    "id": "f2b9e0f1-3de5-4840-ab8a-d89176a4bd74"
   },
   "source": [
    "And repeat the initial configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dddbd68-b3e8-4e35-a9c6-cfe0d12d6ade",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dddbd68-b3e8-4e35-a9c6-cfe0d12d6ade",
    "outputId": "57111746-8c05-4ced-dc9d-c4f2cad614fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = Path(f\"experiments/03/\")\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'in_class': False,          # kept for compatibility with hands-on #1. Leave it as false!\n",
    "    'epochs': 50,               # max epochs for normal trainings\n",
    "    'nas_epochs': 100,          # max epochs for the NAS search loop\n",
    "    'nas_no_stop_epochs': 20,   # initial epochs without early stopping for the NAS\n",
    "    'batch_size': 32,           # batch size\n",
    "    'lr': 0.001,                # initial learning rate for normal trainings\n",
    "    'search_lr_net': 0.001,     # learning rate for DNN weights during NAS\n",
    "    'search_lr_nas': 0.001,     # learning rate for NAS parameters during NAS\n",
    "    'weight_decay': 1e-4,       # weight decay for normal DNN parameters\n",
    "    'patience': 10,             # early-stopping patience for normal trainings\n",
    "    'nas_patience': 10,         # early-stopping patience for NAS search\n",
    "}\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Working on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e410210-8a49-4d48-b0b2-e7061b1c96e4",
   "metadata": {
    "id": "4e410210-8a49-4d48-b0b2-e7061b1c96e4"
   },
   "source": [
    "# Part 1: Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c831a8e-e2b2-4c51-a4f5-ca12ac3ec5ef",
   "metadata": {
    "id": "3c831a8e-e2b2-4c51-a4f5-ca12ac3ec5ef"
   },
   "source": [
    "Dataset preparation is identical to the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b7f59f-1367-4417-9d1d-f100f61d1e8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1b7f59f-1367-4417-9d1d-f100f61d1e8f",
    "outputId": "328ed0a9-884b-4ff4-9d16-0357ba0a98d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "datasets = icl.get_data()\n",
    "dataloaders = icl.build_dataloaders(datasets, batch_size=TRAINING_CONFIG['batch_size'])\n",
    "train_dl, val_dl, test_dl = dataloaders\n",
    "\n",
    "input_shape = datasets[0][0][0].numpy().shape\n",
    "batch_shape = (1,) + input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cFj-TvcpcS6",
   "metadata": {
    "id": "0cFj-TvcpcS6"
   },
   "source": [
    "# Part 2: Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q7rkOr7vwk9r",
   "metadata": {
    "id": "Q7rkOr7vwk9r"
   },
   "source": [
    "All DNN models considered up to now used **32-bit floating point** for internal operations, and for storing weights and activations. However, our hardware target only supports Quantized DNN inference, using **8-bit integers**. Therefore, we need to convert our model to that format before we can export it and compile it.\n",
    "\n",
    "Simply quantizing a model by replacing all floating point data with their closest integer approximation (the most basic form of Post-Training Quantization) could worsen its accuracy. Fortunately, this drop can often be recovered by running some epochs of the so-called **Quantization-Aware Training (QAT)**, as seen in class.\n",
    "\n",
    "PLiNIO an be used to perform QAT on our model, as well as allowing to export the final \"full integer\" model in a format compatible with the compiler used in one of the next sessions.  \n",
    "\n",
    "More precisely, PLiNIO's QAT function is embedded in the `MPS()` class, which is used to perform a more advanced optimization: **Mixed-Precision Search**. This optimization applies QAT at *multiple bit-widths* simultaneously, and uses a SuperNet-like method to select the *best precision assignment* for the weights and activations of different portions of a DNN (different layers, or even different channels of the same layer).\n",
    "The optimization can be driven by a two-terms loss function considering accuracy and cost, similar to the one used with SuperNet and PIT.\n",
    "\n",
    "We will not use MPS in this session, since our target hardware and backend library do not support $<8$ bit inference (*). However, we can still use PLiNIO to perform a simple QAT run, by simply reducing it to a **\"corner case\" of MPS, with a single precision** (8-bit) to select from.\n",
    "\n",
    "If you're interested in the details on the MPS algorithm present in PLiNIO, check-out these two papers: [link1](https://arxiv.org/abs/2206.08852), [link2](https://arxiv.org/abs/2004.05795). Feel free to also try applying MPS with multiple precisions on our DNN as an extra. Although we won't be able to deploy models with precisions different from 8-bit, it could still be interesting to check how much we can compress the weights without losing too much accuracy.\n",
    "\n",
    " \n",
    "(*) Actually, the DNN accelerator present in GAP9 would support those representations, but we will only deploy on the multi-core RISC-V cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f76fa7-0d04-4c96-bba6-34b16bb5c50f",
   "metadata": {},
   "source": [
    "## Importing the Model\n",
    "\n",
    "Let's start by loading the final model from Hands-on #2 (Optimized and Pruned):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b96f8fad-09cb-4280-9c4c-e06a09ffa62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"./experiments/02/final_model.pt\")\n",
    "model = torch.load(MODEL_PATH).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a25cc-4f01-4619-bd7d-de87dae1c737",
   "metadata": {},
   "source": [
    "Quickly verify that it's correctly loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8637a9bb-73c7-4e4b-b468-c6802db13382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 44689, Test Loss: 0.4898137152194977, Test Acc: 83.11000061035156\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_metrics = icl.evaluate(model, criterion, test_dl, device)\n",
    "size = summary(model, batch_shape).total_params\n",
    "print(f'Size: {size}, Test Loss: {test_metrics[\"loss\"]}, Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rp1zVH50-atb",
   "metadata": {
    "id": "rp1zVH50-atb"
   },
   "source": [
    "## Preparing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fx2HOJgy-lBQ",
   "metadata": {
    "id": "Fx2HOJgy-lBQ"
   },
   "source": [
    "The constructor of the `MPS()` class in PLiNIO is similar to the one of PIT. The parameters are similar, and the conversion is mostly transparent. \n",
    "\n",
    "Note that we can ignore the `cost` parameter, if we're interested in just QAT. When running an actual MPS optimization on the DNN weights, you can for instance set this parameter to `params_bit`, a cost model that accounts for the precision (in bits) for each DNN parameter.\n",
    "\n",
    "The only key difference w.r.t. to other methods, is that `MPS` also expect a `qinfo` dictionary, containing settings on the desired type of quantization to apply for different parts of the network.\n",
    "\n",
    "The settings in `qinfo` include the quantization algorithm to use for weights and activations (e.g. min-max, PaCT, etc), and optional configuration parameters. In our case, it suffices to use the reasonable default settings provided by PLiNIO, by calling the `get_default_qinfo()` function. This function expects as input parameters the tuple of weights and activations bitwidths to be included in the optimization (in our case, only 8-bit for both).\n",
    "\n",
    "There's just one thing to customize in the default `qinfo`, namely the range of the DNN **input** quantizer. In fact, since we know that our (float) data is in the $[0, 1]$ range, we can set the initial range of the quantizer to be the same. This should facilitate the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "VImIBMbW-s32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "VImIBMbW-s32",
    "outputId": "a1df1470-78b1-43cb-f65a-32073dc9558e"
   },
   "outputs": [],
   "source": [
    "# get the default qinfo dictionary, specifying 8-bit as the only precision for both weights and activations\n",
    "qinfo = get_default_qinfo((8,), (8,))\n",
    "\n",
    "# modify the default qinfo for the input layer, since we're using signed data in the [0, 1] range\n",
    "qinfo['input_default']['quantizer'] = PACTAct\n",
    "qinfo['input_default']['kwargs'] = {'init_clip_val': +1}\n",
    "\n",
    "# call the PLiNIO constructor\n",
    "mps_model = MPS(model, input_shape=input_shape, qinfo=qinfo)\n",
    "mps_model = mps_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c6af7-1f40-4157-9d18-d91d64ca6721",
   "metadata": {},
   "source": [
    "### Evaluate the Converted Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5094ecc1-eb9f-4334-9f64-9695f5c01963",
   "metadata": {},
   "source": [
    "The model generated by the MPS constructor has approximated weights and operations that simulate int8 precision. Furthermore, other optimizations are performed during the conversion, such as folding Batch Normalization layers with Convolutions or Linear layers, to avoid entirely their execution in the final deployed model. Overall, the result of the conversion is similar to what we would get with a (very basic) post-training quantization. Let's check how this model performs on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2674479d-a358-4a30-993a-4f44fe6488ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4880969524383545, Test Acc: 82.86000061035156\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_metrics = icl.evaluate(mps_model, criterion, test_dl, device)\n",
    "print(f'Test Loss: {test_metrics[\"loss\"]}, Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de89083-5e27-4eb7-a391-7bc4ffe20bec",
   "metadata": {},
   "source": [
    "**Question**: Do you see an accuracy drop after conversion? If not, what does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PI6CEP-PJdnO",
   "metadata": {
    "id": "PI6CEP-PJdnO"
   },
   "source": [
    "## Running QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qb8qLFW8Bti7",
   "metadata": {
    "id": "Qb8qLFW8Bti7"
   },
   "source": [
    "The actual execution of QAT is nothing more than an additional training run, using the model generated by the `MPS()` constructor. Note that, if we wanted to actually *select* the bitwidth using MPS, we would have to run something similar to the `nas_loop` seen in Hands-on #1 and #2. However, we're keeping a fixed precision, and do not aim to optimize the DNN cost (e.g. total memory occupation) in this phase. Or better, we already reduced memory by a factor of 4 by moving from float32 to int8, but now, our goal is just retrieving the (possible) lost accuracy.\n",
    "\n",
    "So, in our case, a simple `training_loop` on the converted model will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c53a25-ef0d-4a09-acea-ac2cac363458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:33<00:00, 37.38batch/s, loss=0.454, acc=84.2, val_loss=0.552, val_acc=80.7]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:33<00:00, 37.22batch/s, loss=0.454, acc=84.3, val_loss=0.556, val_acc=80.9]\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:33<00:00, 37.82batch/s, loss=0.45, acc=84.2, val_loss=0.557, val_acc=80.8]\n",
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:33<00:00, 37.70batch/s, loss=0.453, acc=84.2, val_loss=0.554, val_acc=81]\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.54batch/s, loss=0.453, acc=84.1, val_loss=0.552, val_acc=80.5]\n",
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 37.96batch/s, loss=0.452, acc=84.3, val_loss=0.56, val_acc=80.8]\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 37.94batch/s, loss=0.451, acc=84.1, val_loss=0.557, val_acc=80.3]\n",
      "Epoch 8: 100%|██████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.74batch/s, loss=0.449, acc=84.4, val_loss=0.552, val_acc=81]\n",
      "Epoch 9: 100%|████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.14batch/s, loss=0.446, acc=84.4, val_loss=0.564, val_acc=80.3]\n",
      "Epoch 10: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:33<00:00, 37.69batch/s, loss=0.445, acc=84.4, val_loss=0.561, val_acc=80.5]\n",
      "Epoch 11: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.22batch/s, loss=0.447, acc=84.3, val_loss=0.544, val_acc=81.1]\n",
      "Epoch 12: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.39batch/s, loss=0.444, acc=84.4, val_loss=0.555, val_acc=80.8]\n",
      "Epoch 13: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.29batch/s, loss=0.442, acc=84.4, val_loss=0.544, val_acc=81.1]\n",
      "Epoch 14: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.76batch/s, loss=0.445, acc=84.6, val_loss=0.547, val_acc=80.8]\n",
      "Epoch 15: 100%|█████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:31<00:00, 39.57batch/s, loss=0.446, acc=84.5, val_loss=0.549, val_acc=81]\n",
      "Epoch 16: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:31<00:00, 39.41batch/s, loss=0.444, acc=84.7, val_loss=0.564, val_acc=80.7]\n",
      "Epoch 17: 100%|████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:31<00:00, 39.49batch/s, loss=0.442, acc=84.3, val_loss=0.56, val_acc=80.8]\n",
      "Epoch 18: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 39.05batch/s, loss=0.446, acc=84.4, val_loss=0.561, val_acc=80.7]\n",
      "Epoch 19: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:31<00:00, 39.47batch/s, loss=0.439, acc=84.5, val_loss=0.547, val_acc=80.9]\n",
      "Epoch 20: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:31<00:00, 39.45batch/s, loss=0.439, acc=84.7, val_loss=0.548, val_acc=81.1]\n",
      "Epoch 21: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:33<00:00, 37.75batch/s, loss=0.445, acc=84.5, val_loss=0.552, val_acc=81.2]\n",
      "Epoch 22: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.28batch/s, loss=0.435, acc=84.7, val_loss=0.542, val_acc=81.2]\n",
      "Epoch 23: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:35<00:00, 35.69batch/s, loss=0.438, acc=84.6, val_loss=0.552, val_acc=81.1]\n",
      "Epoch 24: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.20batch/s, loss=0.434, acc=84.6, val_loss=0.561, val_acc=80.5]\n",
      "Epoch 25: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 37.99batch/s, loss=0.437, acc=84.8, val_loss=0.542, val_acc=81.3]\n",
      "Epoch 26: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.38batch/s, loss=0.438, acc=84.6, val_loss=0.545, val_acc=81.1]\n",
      "Epoch 27: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:32<00:00, 38.84batch/s, loss=0.434, acc=84.9, val_loss=0.551, val_acc=80.9]\n",
      "Epoch 28: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:30<00:00, 40.80batch/s, loss=0.434, acc=84.7, val_loss=0.548, val_acc=80.6]\n",
      "Epoch 29: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:30<00:00, 40.67batch/s, loss=0.435, acc=84.9, val_loss=0.544, val_acc=81.2]\n",
      "Epoch 30: 100%|████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:29<00:00, 42.57batch/s, loss=0.432, acc=84.9, val_loss=0.55, val_acc=81.5]\n",
      "Epoch 31: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:20<00:00, 62.14batch/s, loss=0.435, acc=84.6, val_loss=0.547, val_acc=80.8]\n",
      "Epoch 32: 100%|█████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:20<00:00, 62.28batch/s, loss=0.433, acc=84.9, val_loss=0.551, val_acc=81]\n",
      "Epoch 33: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:20<00:00, 61.69batch/s, loss=0.431, acc=84.9, val_loss=0.542, val_acc=81.3]\n",
      "Epoch 34: 100%|█████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:20<00:00, 60.43batch/s, loss=0.435, acc=84.8, val_loss=0.552, val_acc=81]\n",
      "Epoch 35: 100%|█████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:20<00:00, 59.81batch/s, loss=0.431, acc=85, val_loss=0.545, val_acc=80.9]\n",
      "Epoch 36: 100%|██████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:20<00:00, 60.75batch/s, loss=0.43, acc=85, val_loss=0.543, val_acc=81.1]\n",
      "Epoch 37: 100%|█████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:20<00:00, 61.64batch/s, loss=0.428, acc=85, val_loss=0.549, val_acc=80.8]\n",
      "Epoch 38: 100%|████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:20<00:00, 61.84batch/s, loss=0.431, acc=84.9, val_loss=0.54, val_acc=81.5]\n",
      "Epoch 39: 100%|███████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:20<00:00, 60.35batch/s, loss=0.432, acc=84.9, val_loss=0.541, val_acc=81.3]\n",
      "Epoch 40: 100%|███████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:43<00:00, 28.99batch/s, loss=0.427, acc=85, val_loss=0.542, val_acc=81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping!\n",
      "Stopped at epoch 40 because of early stopping\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.defs.I_SuperNet import training_loop\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "history = training_loop(SAVE_DIR / 'qat', TRAINING_CONFIG, mps_model, criterion, train_dl, val_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7b18a-7fa9-42c8-9513-758943e397ea",
   "metadata": {
    "id": "2ee795d5-cadb-45fe-92db-1cceb1638ffb"
   },
   "source": [
    "### Evaluating the Model after QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6c8bc-85a5-42fb-8099-d916ef2b7274",
   "metadata": {},
   "source": [
    "Let's test the final model after QAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7625e43f-b8d0-462c-85a0-af8871fc04b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c4089ce-bfe8-4768-8456-e44d6c37b684",
    "outputId": "279110db-8205-4391-b1c6-c14bd11df777",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model cost: 344568.0, Test Loss: 0.49479252099990845, Test Acc: 83.13999938964844\n"
     ]
    }
   ],
   "source": [
    "test_metrics = icl.evaluate(mps_model, criterion, test_dl, device)\n",
    "print(f'Final model cost: {mps_model.cost}, Test Loss: {test_metrics[\"loss\"]}, Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3344c2c-083c-4470-b594-1d69d5715fce",
   "metadata": {},
   "source": [
    "After QAT, it is possible that your model has become even **slightly more accurate** than the floating point version! This sometimes happens when quantizing: the approximation introduced by quantization has a *regularizing* effect, which makes the model behave slightly better on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aRJOELwKSP2",
   "metadata": {
    "id": "6aRJOELwKSP2"
   },
   "source": [
    "# Part 3: Export for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51aea75-7f0a-476a-829c-9af39d2850ab",
   "metadata": {},
   "source": [
    "We can now call the `.export()` method of the PLiNIO MPS model, exactly as done for SuperNet and PIT. Notice however, that in this case, the exporting phase has a slightly different behaviour. In fact, rather than outputting a model that includes standard torch layers, we replace each quantized layer with a new class (for instance, `nn.Conv2D` becomes `QuantConv2D`). These layers function analogously to the torch equivalents, but also store the quantization parameters (e.g. min/max values for each weight tensor), and use them to simulate the effect of quantization during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d835e3e8-697e-4bed-aef5-092cbf5941d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mps_model = copy.deepcopy(mps_model)\n",
    "quant_model = final_mps_model.export().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Ahd0um3Kxkt",
   "metadata": {
    "id": "3Ahd0um3Kxkt"
   },
   "source": [
    "Moreover, the model exported by MPS still only applies a so-called \"fake quantization\". This means that the DNN is not yet using *only* integer data. Rather, some parameters, such as the scale factors and zero offsets for (re-)quantization are still in floating point. To deploy on our target, however, all data should be integer. For instance, in the PULP-NN backend library that we will use on our target, the multiplication times a floating point scaling factor is replaced by the sequence of: i) an integer multiplication and ii) a right shift.\n",
    "\n",
    "To obtain a model that fully complies with this execution model, we need a further conversion step. This is implemented by the next cell, which calls the `integerize_arch` function with parameters that specify the desired backend, among the supported ones, and some other optional parameters. The backend essentially refers to the compiler that will be used to take the model and convert it to inference code for the hardware. In our case, it will be the [MATCH](https://github.com/eml-eda/match) compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "MhDpsvm8K0b0",
   "metadata": {
    "id": "MhDpsvm8K0b0"
   },
   "outputs": [],
   "source": [
    "# convert the model to full-integer, compiler-compliant format\n",
    "full_int_model = integerize_arch(quant_model, Backend.MATCH, backend_kwargs={'shift_pos': 32})\n",
    "full_int_model = full_int_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FuXDsNoeK650",
   "metadata": {
    "id": "FuXDsNoeK650"
   },
   "source": [
    "Since the previous conversion removes all floating point operations from the network, it might affect the accuracy. So, let's verify by how much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "DVFyXZ7nLTky",
   "metadata": {
    "id": "DVFyXZ7nLTky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2342.680419921875, Test Acc: 80.40999603271484\n"
     ]
    }
   ],
   "source": [
    "test_metrics = icl.evaluate(full_int_model, criterion, test_dl, device)\n",
    "print(f'Test Loss: {test_metrics[\"loss\"]}, Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I3GVZA5fL2zr",
   "metadata": {
    "id": "I3GVZA5fL2zr"
   },
   "source": [
    "We now have to save the final model. In this case, differently from Hands-on #1 and #2, we don't need to save it in PyTorch format. Rather, we generate an ONNX file compatible with what the MATCH compiler expects. The following cell does that. \n",
    "\n",
    "It essentially generates the ONNX using torch's built-in utility, and then adds some custom annotations as desired by MATCH. Let's run it (you can safely ignore the warnings that appear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "KyC4xItHLTk0",
   "metadata": {
    "id": "KyC4xItHLTk0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jahier/oenne/oenne_venv/lib/python3.10/site-packages/plinio/methods/mps/quant/backends/match/nn/conv2d.py:185: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return torch.tensor(0., device=self.device)\n",
      "/home/jahier/oenne/oenne_venv/lib/python3.10/site-packages/plinio/methods/mps/quant/backends/match/nn/conv2d.py:190: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return torch.tensor(2 ** cast(int, self.out_quantizer.precision) - 1, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "exporter = MATCHExporter()\n",
    "exporter.export(copy.deepcopy(full_int_model), batch_shape, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b8eff-c48c-4c48-8f28-7af3e2cc6a69",
   "metadata": {},
   "source": [
    "Last but not least, we also have to save the \"clip value\" used by our quantized model to convert inputs (originally real values in [0, 1]) to 8-bit integers. \n",
    "\n",
    "Namely, the MPS class in PLiNIO (and the following integerization step) use an \"Input Quantizer\" layer to perform this transformation. The Input Quantizer is of type `PACTAct`, as specified in the `qinfo` data structure above. This type of quantizer learns a clipping (i.e. saturation) value for the input tensor. It then rescales the data between 0 and this value to occupy the full int8 range. Mathematically, the operation performed, for unsigned 8-bit data, is the following:\n",
    "\n",
    "$$X_{int} = \\left\\lfloor \\frac{255}{\\alpha} \\cdot \\min(X_{float}, \\alpha) \\right\\rfloor $$\n",
    "\n",
    "\n",
    "where $\\alpha$ is the learned clipping value. At the beginning of our QAT, we initialized $\\alpha = 1$. However, this value is trainable, so it could have changed during the QAT execution. So, let's extract the clipping value from the full integer model, and save it on disk for later usage. We will use a simple JSON dictionary for this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e2e65c3-938d-4c36-9e53-8665c8e7f6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input clipping value: 1.0000581741333008\n"
     ]
    }
   ],
   "source": [
    "qtz = full_int_model.input_1_input_quantizer\n",
    "clip_val = float(qtz.out_quantizer.clip_val)\n",
    "print(f\"Input clipping value: {clip_val}\")\n",
    "\n",
    "scaling_dict = {'clip_val': clip_val}\n",
    "with open(f\"{SAVE_DIR}/rescaling_values.json\", \"w\") as f:\n",
    "    json.dump(scaling_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73490aa3-7b95-42fb-9030-1d5ef5a981bc",
   "metadata": {},
   "source": [
    "You will probably see that the clipping value remained very close to the initialization value (+1). This simply means that our model has learned to avoid any clipping, and map the entire input range to the uint8 range. We kept this last step for generality, as it could be very important on other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26c0b2-335d-4d25-8f80-16ba9198b0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
